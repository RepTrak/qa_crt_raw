{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0d9512",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Automation 2.0 - JSON and SPSS\n",
    "\n",
    "\n",
    "This notebook outlines the core functions of the new automation method.\n",
    "\n",
    "\n",
    "Andrew Caide.    \n",
    "RepTrak Company.    \n",
    "November 20222\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "This first method below (`run` from `utils.main`) will execute the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5084a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.main import run\n",
    "S3_PATH = 's3://reptrak-perception-data/'\n",
    "\n",
    "start = 1\n",
    "def stage_gen(start=start):\n",
    "    while True:\n",
    "        yield(start)\n",
    "        start += 1\n",
    "\n",
    "resume=start\n",
    "stage = stage_gen(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35f60f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Redshift | 2022-11-01 19:25:17,401 | botocore.credentials | INFO | Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Reading file from: s3://reptrak-perception-data/Raw_Data/Zip_Files/Input/\n",
      "Tempoary processing file location: ./tmp/qa_data.zip\n",
      "---\n",
      ".sav files found!\n",
      "Working with the .sav files: ['./tmp/qa_data/qa_data.sav']\n",
      ".json files found!\n",
      "Working with the .json files: ['./tmp/qa_data/j_subset.json']\n",
      ".txt files found!\n",
      "Working with the .txt files: ['./tmp/qa_data/CRT_Sept_JSON_Final Data Map.txt']\n",
      "---\n",
      "\n",
      "Passing dataset from s3 (s3://reptrak-perception-data/Raw_Data/Zip_Files/Input/qa_data.zip) to ec2 (tmp) and unzipping\n",
      "download: s3://reptrak-perception-data/Raw_Data/Zip_Files/Input/qa_data.zip to tmp/qa_data.zip\n",
      "Attempting to extract using 7za\n",
      "7za failed, using 'unzip'\n",
      "Archive:  ./tmp/qa_data.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "replace ./tmp/qa_data/qa_data.sav? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n",
      "(EOF or read error, treating as \"[N]one\" ...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening SPSS file ./tmp/qa_data/qa_data.sav via pyreadstat.multiprocessing().\n",
      "DI-SPSS: Identified 5720 columns to be removed from SPSS data.\n",
      "DI-SPSS: Size of dataframe before removing junk: (1000, 8662)\n",
      "DI-SPSS: Size of dataframe after removing junk: (1000, 4307)\n",
      "DI-SPSS: Stacking SPSS Data!\n",
      "---\n",
      "SPSS: Original Shape of dataframe: (1000, 4308)\n",
      "SPSS: Getting valid integers from metadata\n",
      "SPSS: Converting all columns to Int64: []\n",
      "SPSS: Identified 0 that need to be converted to integers\n",
      "SPSS: Dropping/Renaming custom questions from df2 (109 total)\n",
      "SPSS: Shape of dataframes pre-merging:\n",
      "1: (1000, 3767), 2: (1000, 3779)\n",
      "SPSS: Shape of final dataframe: (2000, 3779)\n",
      "---\n",
      "\n",
      "---\n",
      "Opening all JSON files ['./tmp/qa_data/j_subset.json'].\n",
      "---\n",
      "JSON: Turning json (j_subset.json, 1-by-1) (len: 1000) to pandas dataframe!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/io/data_intake_spss.py:227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_aus['INDUSTRY_INDEX1'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON: Getting valid integers from metadata\n",
      "JSON: Converting all columns to Int64\n",
      "JSON: Identified 1632 that need to be converted to integers\n",
      "JSON: Data conversion complete on 1000th column, Q_L_698_001A_3\n",
      "JSON: Not saving JSON.\n",
      "JSON: Stacking dataframe (current shape: (1000, 3137))\n",
      "JSON: Original Shape of dataframe: (1000, 3137)\n",
      "JSON: Dropping/Renaming custom questions from df2 (883 total)\n",
      "---\n",
      "JSON: Shape of dataframes pre-merging:\n",
      "1: (1000, 2115), 2: (1000, 2269)\n",
      "JSON: Checking columns for stuff that should not be there.\n",
      "JSON: Data loaded. (shape: (2000, 2269))\n",
      "---\n",
      "---\n",
      "JSON: respStatus\n",
      "dtype:\n",
      "Int32\n",
      "values:\n",
      "3    1008\n",
      "1     524\n",
      "4     468\n",
      "Name: respStatus, dtype: Int64\n",
      "---\n",
      "respStatus Count:\n",
      "2    1008\n",
      "1     524\n",
      "4     468\n",
      "Name: respStatus, dtype: Int64\n",
      "JSON: Deleting NA country cases! (shape: 2000)\n",
      "JSON: Final shape: 1980\n",
      "JSON: Correcting Stakeholder Flags!\n",
      "Stakeholder_PetOwner1(stakeholder_var) - False Positive rate: 0.47\n",
      "Remaining issues: 0.00\n",
      "---\n",
      "Stakeholder_Influencer1(stakeholder_var) - False Positive rate: 0.55\n",
      "Remaining issues: 0.00\n",
      "---\n",
      "Stakeholder_HH_DM1 has 0 cases.\n",
      "Stakeholder_BizOwner1 has 0 cases.\n",
      "Stakeholder_BankAcctOwner1 has 0 cases.\n",
      "Stakeholder_CarOwner1 has 0 cases.\n",
      "Stakeholder_US_CA1(stakeholder_var) - False Positive rate: 0.30\n",
      "Remaining issues: 0.00\n",
      "---\n",
      "Stakeholder_US_FL1(stakeholder_var) - False Positive rate: 0.50\n",
      "Remaining issues: 0.00\n",
      "---\n",
      "Stakeholder_US_GA1 has 0 cases.\n",
      "Stakeholder_US_IL1 has 0 cases.\n",
      "Stakeholder_US_NY1 has 0 cases.\n",
      "Stakeholder_US_NC1 has 0 cases.\n",
      "Stakeholder_US_OH1 has 0 cases.\n",
      "Stakeholder_US_PA1 has 0 cases.\n",
      "Stakeholder_US_TX1 has 0 cases.\n",
      "Stakeholder_US_NJ1(stakeholder_var) - False Positive rate: 0.60\n",
      "Remaining issues: 0.00\n",
      "---\n",
      "Stakeholder_ITDM1 not in the dataframe\n",
      "Stakeholder_AU_NSW1 has 0 cases.\n",
      "Stakeholder_AU_QLD1 has 0 cases.\n",
      "Stakeholder_AU_VIC1 has 0 cases.\n",
      "Stakeholder_CA_QU1 has 0 cases.\n",
      "Stakeholder_BR_MG1 has 0 cases.\n",
      "---\n",
      "JSON: Stakeholders have been corrected. Fixing IGP\n",
      "---\n",
      "hSampleType dtype: Int32\n",
      "Stakeholder_Sum: Int32\n",
      "JSON: Fixing internal research flags.\n",
      "JSON: Done cleaning JSON file!\n",
      "---\n",
      "\n",
      "Appending SPSS data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/io/data_intake_main.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_spss['File_Origin'] = 'SPSS'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape with only SPSS (au) data: (2000, 3781)\n",
      "Appending JSON data.\n",
      "Dataframe shape after appending JSON data: (3980, 5610)\n",
      "data_intake_main.py: Transforming <NA> to np.nan for critical columns:\n",
      "['Q305_1', 'Q305_2', 'Q305_3', 'Q305_4', 'Q320_1', 'Q320_2', 'Q320_3', 'Q320_4', 'Q320_5', 'Q320_6', 'Q320_7', 'Q320_8', 'Q320_9', 'Q320_10', 'Q320_11', 'Q320_12', 'Q320_13', 'Q320_14', 'Q320_15', 'Q320_16', 'Q320_17', 'Q320_18', 'Q320_19', 'Q320_20', 'Q320_21', 'Q320_22', 'Q320_23', 'Q320_24', 'Q320_30', 'Q320_37', 'Q320_38', 'Q320_39', 'Q320_999', 'Q800_2', 'Q800_3', 'Q800_4', 'Q800_5', 'Q800_6', 'Q800_7', 'Q800_8', 'Q800_9', 'Q800_17', 'Q800_50', 'Q800_51', 'Q800_52', 'Q800_53', 'Q305CEO_1', 'Q320A_Selected', 'Q3201P_1', 'Q3201P_2', 'Q3201P_3', 'Q3200P_2', 'Q3200P_1', 'Q3200P_3']\n",
      "Working on json metadata.\n",
      "Number of columns that exist in the dataframe (cols: 5610) ALSO in meta: 2767\n",
      "Number of columns that DON'T exist in the dataframe ALSO in meta: 9409\n",
      "Found 7646 new variables.\n",
      "Found 1016 variables already in the data.\n",
      "FILTERS: Size of data before deleting missing familiarities: (3980, 5317)\n",
      ":::::::::::::::::::::::::::::::::::::::::\n",
      "::::::: Operating on Dynata file. :::::::\n",
      ":::::::::::::::::::::::::::::::::::::::::\n",
      "Size of data before dropping missing 'COUNTRY' cases: (3980, 5317)\n",
      "Size of data after dropping missing 'COUNTRY' cases: (3976, 5317)\n",
      "Size of data before dropping missing 'Rating' cases: (3976, 5318)\n",
      "Size of data after dropping missing 'Rating' cases: (2863, 5318)\n",
      "FILTERS-Fam: Creating Fam_Company\n",
      "FILTERS-Fam: Could not find the following S105 columns: []\n",
      "FILTERS-Fam: Dropping cases of missing familiarity columns: (2863, 5319)\n",
      "FILTERS-Fam: After dropping cases of missing familiarity columns: (2863, 5319)\n",
      "FILTERS-Fam: Running Familiarity-finder\n",
      "FILTERS-Fam: Setting Familiarity\n",
      "FILTERS-Fam: [After dropping S105s] DF Size Check: 0.16GB\n",
      "FILTERS-Fam: After dropping unfamiliar ratings: 2855, unique values: [4. 7. 5. 6.]\n",
      "FILTERS: Size of data after familiarity filters: (2855, 3808)\n",
      "FILTERS-Demo: Unique respStatus:\n",
      "<FloatingArray>\n",
      "[3.0, 2.0, 1.0, 4.0]\n",
      "Length: 4, dtype: Float64\n",
      "FILTERS-Demo: Before dropping respStatus != 2: 2855\n",
      "FILTERS-Demo: After dropping respStatus != 2: 2066\n",
      "FILTERS-Demo: BEFORE DROP - Unique ages: <FloatingArray>\n",
      "[5.0, 4.0, 3.0, 2.0, 1.0]\n",
      "Length: 5, dtype: Float64\n",
      "FILTERS-Demo: AFTER DROP - Unique ages: <FloatingArray>\n",
      "[5.0, 4.0, 3.0, 2.0, 1.0]\n",
      "Length: 5, dtype: Float64\n",
      "FILTERS-Demo: Shape after dropping Ages (0 and 5's): (2066, 3808)\n",
      "---\n",
      "FILTERS-Demo: Unique Pulse Missings: [0 3 1 2 4]\n",
      "FILTERS: Size of data after demographic filters: (1982, 3809)\n",
      "\n",
      "===================================\n",
      "=      Processing Awareness       =\n",
      "===================================\n",
      "\n",
      "awareness_main.py: Reading raw data and extracting awareness columns.\n",
      "['CAR', 'ITDM', 'OPINION_INFLUENCER', 'ACCOUNT', 'PET_OWNER', 'HHDM', 'Region_C', 'hSampleType', 'BUSINESS_OWNER']\n",
      "get_awareness.py: Opening SPSS file ./tmp/qa_data/qa_data.sav via pyreadstat.multiprocessing().\n",
      "hSampleType found in AUS SPSS data.\n",
      "get_awareness.py: Renaming S105Fieldeda to S105Fielded (AUS)\n",
      "---\n",
      "get_awareness.py: Opening all JSON files ['./tmp/qa_data/j_subset.json'].\n",
      "get_awareness.py: Reading and normalizing ./tmp/qa_data/j_subset.json\n",
      "get_awareness.py: Appending json to awareness data.\n",
      "get_awareness.py: DF Shape after json appending: (2000, 763).\n",
      "get_awareness.py: Finished creating awareness dataframe.\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for hSampleType\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for PET_OWNER\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for OPINION_INFLUENCER\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{}\n",
      "\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4}\n",
      "\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5}\n",
      "\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6}\n",
      "\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 8}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 9}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 10}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 11}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 12}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 13}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 14}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 15}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 16}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 17}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7}\n",
      "\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 17}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7, 'ITDM': 18}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 17}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7, 'ITDM': 18}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 17}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7, 'ITDM': 18}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Redshift | 2022-11-01 19:26:36,281 | botocore.credentials | INFO | Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 17}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7, 'ITDM': 18}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 22}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7, 'ITDM': 18}\n",
      "\n",
      "awareness_main.py: Found stakeholder variables; Computing Awareness for Region_C\n",
      "Stakeholder variables found in the data but no cases identified: \n",
      "{'PET_OWNER': 2, 'Region_C': 23}\n",
      "\n",
      "Stakeholder variables completely missing from the data: \n",
      "{'HHDM': 4, 'BUSINESS_OWNER': 5, 'ACCOUNT': 6, 'CAR': 7, 'ITDM': 18}\n",
      "\n",
      "awareness_main.py: Awareness computations complete. Cleaning up results.\n",
      " --- \n",
      "Checking Awareness:\n",
      "   stakeholder_id  user_count\n",
      "0               1        2032\n",
      "1               3        2232\n",
      "2              19        1952\n",
      "3              20        1872\n",
      "4              21        1872\n",
      " --- \n",
      "8\n",
      "<class 'int'>\n",
      "Pre-upload data cleaning.\n",
      "Working with the following column names: ['Rating', 'familiarity_id', 'user_count', 'familiarity_percent', 'familiarity_label', 'stakeholder_id', 'Year', 'Month', 'Country', 'Company']\n",
      "Final structure:\n",
      "    year  month  country_id  rating_id  company_id familiarity_id  familiarity_percent familiarity_label  user_count  stakeholder_id\n",
      "72  2022      8          26     260002        1306              1                 0.97      Not Familiar           1               1\n",
      "73  2022      9          26     260002        1306              1                 0.97      Not Familiar           1               1\n",
      "Uploading! Filename: NT Awareness Aug, 2022 (Nov 01, 2022 19:26:36)\n",
      "NT Awareness Aug, 2022 (Nov 01, 2022 19:26:36) saved to s3://reptrak-perception-data/Complete/Awareness_Data.\n",
      "Uploading complete.\n",
      "===================================\n",
      "=        Awareness COMPLETE       =\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n",
      "/Users/andrew/Desktop/Work Studies/_CRT - Processing QA/utils/computations/compute.py:124: RuntimeWarning: Mean of empty slice\n",
      "  return(np.nanmean(np.ma.masked_values(vals,99),axis=1).data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining codebook with data...\n",
      "Non-matching codebook countries:set()\n",
      "Join complete.\n",
      "-------\n",
      "Computing scores.\n",
      "Computing Logistics\n",
      "\n",
      "---\n",
      "CRT Month and year: 8, 2022\n",
      "---\n",
      "Missing an age group in country: 198 with gender 1.0, age 1\n",
      "\n",
      "---\n",
      "Computing Weights via Python.\n",
      "---\n",
      "Settting up quantipy data for country 26.0.\n",
      "Inferring meta data from pd.DataFrame.columns (3092)...\n",
      "Converted 3092 columns!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:310: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  self._df[self._weight_name()] = pd.np.zeros(len(self._df))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:505: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  self.pre_weight = pd.np.ones(len(self.dataframe))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:521: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  if pd.np.isnan(self.dataframe[self.weight_column_name]).sum() > 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.482621\n",
      "---\n",
      "Settting up quantipy data for country 198.0.\n",
      "Inferring meta data from pd.DataFrame.columns (3092)...\n",
      "Converted 3092 columns!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:310: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  self._df[self._weight_name()] = pd.np.zeros(len(self._df))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:505: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  self.pre_weight = pd.np.ones(len(self.dataframe))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:521: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  if pd.np.isnan(self.dataframe[self.weight_column_name]).sum() > 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.399276\n",
      "---\n",
      "Settting up quantipy data for country 27.0.\n",
      "Inferring meta data from pd.DataFrame.columns (3092)...\n",
      "Converted 3092 columns!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:310: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  self._df[self._weight_name()] = pd.np.zeros(len(self._df))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:505: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  self.pre_weight = pd.np.ones(len(self.dataframe))\n",
      "/opt/anaconda3/lib/python3.9/site-packages/quantipy/core/weights/rim.py:521: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  if pd.np.isnan(self.dataframe[self.weight_column_name]).sum() > 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.425186\n",
      "---\n",
      "3-month rolling has been skipped; not renaming orig_ columns.\n",
      "Length of smean_names, smean_all: 56, 56\n",
      "Duplicate Columns? []\n",
      "Computing scores for constructs.\n",
      "Computing Touchpoints.\n",
      "All basic computations complete!\n",
      "=======================\n",
      "\n",
      "\n",
      "Duplicate Columns? ['start_date']\n",
      "\n",
      "Checking Industries:\n",
      "<IntegerArray>\n",
      "[0, 1]\n",
      "Length: 2, dtype: Int64\n",
      "<FloatingArray>\n",
      "[1010.0, 2530.0, 4020.0, 2550.0, 5010.0, 5510.0, 4010.0, 2030.0, 4030.0,\n",
      " 4520.0, 3010.0, 5020.0, 3020.0, 2010.0, 2020.0, 3520.0, 2510.0, 3030.0,\n",
      " 4530.0, 4510.0, 3510.0, 2520.0, 6010.0, 1510.0]\n",
      "Length: 24, dtype: Float64\n",
      "Applying Stakeholder fix.\n",
      "original df: (1982, 3216)\n",
      "final_df: (875, 3217)\n",
      "Final_DF.shape: (2004, 3217)\n",
      "***\n",
      "Final dataset size: (2004, 361)\n",
      "***\n",
      "Duplicate Columns? []\n",
      "---\n",
      "Unique stakeholders:\n",
      "stakeholder_id\n",
      "1     1913\n",
      "2        7\n",
      "3       73\n",
      "8        5\n",
      "9        4\n",
      "17       2\n",
      "Name: global_id, dtype: int64\n",
      "---\n",
      "We probably have issues with these columns: []\n",
      "***\n",
      "Final dataset size: (2004, 361)\n",
      "***\n",
      "NT Aug, 2022 (Nov 01, 2022 19:27:05)_New Format saved to s3://reptrak-perception-data/Complete/General_Data_New_Format.\n",
      "imputation.py: Data shape: (1982, 3210)\n",
      "imputation.py: Battery imputation completed in 0:00:11.198902\n",
      "imputation.py: Running imputations on Q215\n",
      "imputation.py: Battery imputation completed in 0:00:05.045129\n",
      "imputation.py: Running imputations on Q410\n",
      "imputation.py: Battery imputation completed in 0:00:07.526156\n",
      "imputation.py: Running imputations on Q800\n",
      "imputation.py: Battery imputation completed in 0:00:08.494133\n",
      "imputation.py: Fixing out of bounds results in imputations.\n",
      "imputation.py: Skipping dep_var_imp\n",
      "imputation.py: Imputations are complete.\n",
      "Applying Stakeholder fix.\n",
      "original df: (1982, 3303)\n",
      "final_df: (875, 3303)\n",
      "Warning! No cases (rating+stakeholder_id) >= 100!\n",
      "***\n",
      "Imputed dataset size: (2004, 68)\n",
      "Upload skipped! (OFF)\n",
      "***\n",
      "Typecast: Could not find column company_type\n",
      "Last check for missing columns\n",
      "The following columns not found in final_df: ['q215_2_imp', 'q215_9_imp', 'q410_4_imp', 'q410_9_imp', 'q410_10_imp', 'relevance_imp', 'informativeness_imp', 'q600_17', 'q600_23', 'q600_25', 'q600_30', 'q600_31']\n",
      "***\n",
      "New imputed dataset size: (2004, 126)\n",
      "Upload skipped! (OFF)\n",
      "***\n",
      "Saving csv with correct modeling samples: SPSS_qa_data.zip_Final Results.csv\n",
      "Zipping results: ./Sep2022_CRT Results.zip\n",
      "Attempting to extract using 7za\n",
      "7za failed, using 'zip'\n",
      "updating: SPSS_qa_data.zip_Final Results.csv (deflated 92%)\n",
      "Deleting unzipped df.\n",
      "Saving and zipping .sav file: qa_data.zip_Final Results.csv.sav\n",
      "Zipping results ./Sep2022_SAV Results.zip\n",
      "Attempting to extract using 7za\n",
      "7za failed, using 'zip'\n",
      "updating: qa_data.zip_Final Results.csv.sav"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Redshift | 2022-11-01 19:28:04,692 | botocore.credentials | INFO | Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (deflated 96%)\n",
      "Deleting unzipped df.\n",
      "Uploading ./Sep2022_CRT Results.zip to SPSS_CSV_Files/Sep2022_CRT Results.zip\n",
      "Uploading ./Sep2022_SAV Results.zip to SPSS_CSV_Files/Sep2022_SAV Results.zip\n",
      "Archiving data file from s3://reptrak-perception-data/Raw_Data/Zip_Files/Input/qa_data.zip to s3://reptrak-perception-data/Raw_Data/Zip_Files/Archive/qa_data.zip\n",
      "Deleting temp files.\n",
      "Deleting: ./tmp/qa_data.zip\n",
      "Data has completely processed.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "run(S3_PATH,\n",
    "    start = 0, \n",
    "    stop = False, \n",
    "    upload = False, \n",
    "    roll = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc593668",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Modular Components.\n",
    "\n",
    "The 7 cell blocks below are the individual steps of the automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff17361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS: COMPLETE\n",
    "from utils.io.setup import get_logistics\n",
    "\n",
    "# For fun :) \n",
    "start = 0\n",
    "def stage_gen(start=start):\n",
    "    while True:\n",
    "        yield(start)\n",
    "        start += 1\n",
    "\n",
    "stage = stage_gen(start)\n",
    "\n",
    "paths_dic, codebook_df, constructs_df, export_cmd = get_logistics('s3://reptrak-perception-data/')\n",
    "stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS: COMPLETE\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#                               Processing Steps\n",
    "from utils.io.io import Unzip, transfer_from_s3\n",
    "\n",
    "if next(stage)==0:\n",
    "    print(\"Passing dataset from s3 ({}) to ec2 (tmp)\".format(paths_dic['filename_with_path']))\n",
    "    msg = transfer_from_s3(export_cmd)\n",
    "    print(\"Unzipping file\")\n",
    "    Unzip(paths_dic['tmp_path'])\n",
    "    if stop:\n",
    "        stage = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS: COMPLETE!\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#       STAGE 1.a: Loading + Cleaning\n",
    "import json\n",
    "from utils.io.data_intake_main import get_files_dic, etl\n",
    "\n",
    "if next(stage) == 1:\n",
    "    ALL_FILES = get_files_dic('./tmp/qa_data/')\n",
    "    json_meta = json.load(open(ALL_FILES['.txt'][0]))\n",
    "    df, meta, SPSS, JSON = etl(ALL_FILES, json_meta)\n",
    "    df = filters(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54807ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS: COMPLETE!\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#       STAGE 2: Awareness\n",
    "\n",
    "from utils.awareness.awareness_main import awareness\n",
    "\n",
    "adf = awareness(df, ALL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fbc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS: Complete!\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#       STAGE 3: Computations\n",
    "\n",
    "#import importlib\n",
    "#import utils.computations.compute\n",
    "#importlib.reload(utils.computations.compute)\n",
    "\n",
    "from utils.computations.compute import compute\n",
    "\n",
    "df = compute(df, codebook_df, constructs_df, upload = False, roll_cmd = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS: Complete!\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#       STAGE 4: Upload scores and distributions to analyzer\n",
    "\n",
    "from utils.upload.upload_platform_file import send_to_analyzer\n",
    "\n",
    "send_to_analyzer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS: Complete!\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#       STAGE 5: Imputations\n",
    "\n",
    "from utils.computations.imputation import run_imputations\n",
    "#import utils.computations.imputation\n",
    "#importlib.reload(utils.computations.imputation)\n",
    "\n",
    "df_imp = run_imputations(df, constructs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13587dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATUS: Under Constructions\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#       STAGE 6: Upload Imputations\n",
    "\n",
    "from utils.upload.upload_imputation import upload_imp\n",
    "\n",
    "zip_filenames = upload_imp(df_imp[0], upload_cmd=False, paths_dic=paths_dic, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io.clean_up import clean_up\n",
    "\n",
    "clean_up(zip_filenames, paths_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b8c03",
   "metadata": {},
   "source": [
    "fin. \n",
    "\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
